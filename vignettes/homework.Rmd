---
title: "All Homework"
author: "Wenyi Li"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{All Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## HW0

## Question

Use knitr to produce 3 examples in the book. The 1st example should contain texts and at least one ﬁgure. The 2nd example should contains texts and at least one table. The 3rd example should contain at least a couple of LaTeX formulas.

## Answer

The 1st example:

```{r echo=FALSE}
x<--18:18
y<-x^+2
plot(x,y,main="Quadratic function image",xlab="x",ylab="y",type="o")

```

The 2st example:

```{r echo=FALSE}
library(knitr)
kable(head(mtcars[1:6]))
```

The 3st example:

$$X\sim U(a,b),f(x)=\frac{1}{b-a}$$
$$X\sim N(\mu,\sigma^{2}),f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{\sigma^2}}$$
$$x\sim Exp(\lambda),f(x)=\lambda e^{-\lambda x}$$

## HW1

# Exercises 3.3:
Because $F(x)=1-(\frac{b}{x})^{a},x\geq b>0,a>0$, $F^{-1}(u)=\frac{b}{(1-u)^{\frac{1}{a}}}$

```{r , echo=FALSE}
n <- 1200
u <- runif(n)
x <- 2/(1-u)^{1/2}
hist(x,prob=TRUE,main=expression(Pareto(2,2)))
y <- seq(0,120, .01)
lines(y,8/y^3)
```

# Exercises 3.9:
```{r echo=FALSE}
n<-1e5
u1<-runif(n,-1,1)
u2<-runif(n,-1,1)
u3<-runif(n,-1,1)
for(i in 1:n)
{
  if (abs(u3[i])>=abs(u2[i])&abs(u3[i])>=abs(u1[i])){x[i]<-u2[i]}
  else {x[i]<-u3[i]}  
}
hist(x,prob=TRUE,main=expression(histogram ))
```

# Exercises 3.10:
Because$u_{1},u_{2},u_{3}$ i.i.d $\sim U(-1,1)$,so $f(u_{i})=\frac{1}{2}$,and
$$X=\left\{\begin{matrix}
u_{2}, &|u_{3}|\geq |u_{2}|且|u_{3}|\geq |u_{1}|\\ 
u_{3}, & otherwise. 
\end{matrix}\right.$$,Hence:
$$
\begin{aligned}
F(x)&=P(X\leq x)\\&=P(u_{2}\leq x,|u_{3}|\geq |u_{2}|且|u_{3}|\geq |u_{1}|)+P(u_{3}\leq x,otherwise)\\&=\frac{1}{8}[\int_{-1}^{x}du_{2}\int_{|u_{3}|\geq |u_{2}|}du_{3}\int_{|u_{3}|\geq |u_{1}|}du_{1}]+
[\int_{-1}^{x}\frac{1}{2}du_{3}-\frac{1}{8}\int_{-1}^{x}du_{3}\int_{|u_{3}|\geq |u_{2}|}du_{2}\int_{|u_{3}|\geq |u_{1}|}du_{1}]
\\&=\frac{1}{8}[\int_{-1}^{x}du_{2}\int_{|u_{3}|\geq |u_{2}|}2|u_{3}|du_{3}]
+\frac{1}{2}(x+1)-\frac{1}{2}\int_{-1}^{x}(u_{3})^{2}du_{3}
\\&=\frac{1}{4}\int_{-1}^{x}1-(u_{2})^{2}du_{2}+\frac{1}{2}(x+1)-\frac{1}{6}(x^{3}+1)
\\&=\frac{1}{4}(x+\frac{2}{3}-\frac{x^{3}}{3})+\frac{1}{2}(x+1)-\frac{1}{6}(x^{3}+1)
\\&=\frac{3x}{4}-\frac{x^{3}}{4}+\frac{1}{2}
\end{aligned}
$$
Hence,$$f(x)=F^{'}(x)=\frac{3}{4}(1-x^{2})$$


# Exercises 3.13:
```{r pressure, echo=FALSE}
n<- 1e4
r<-4
beta<-2
lambda<-rgamma(n,r,beta)
x<-rexp(n,lambda)
hist(x,prob=TRUE,main=expression(comparison))
y<- seq(0,80, .01)
lines(y,8/y^3)

```


## HW2

# Exercises 5.1:
Compute a Monte Carlo estimate of $\int_{0}^{\frac{\pi}{3}}sintdt$,and compare your estimate with the exact value of the integral.
```{r }
n<-1e5
x<-runif(n,min=0,max=pi/3)
theta.hat<-mean(sin(x))*pi/3
theta<-cos(0)-cos(pi/3)
print(c(theta.hat,theta))

```

# Exercises 5.7:
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $θ$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.

simple Monte Carlo method:$$θ=\int_0^1e^xdx=E[e^x],x\sim U(0,1)$$
$$\hatθ=\frac{1}{n}\sum_{i=1}^{n}e^{x_i}$$
antithetic variate approach:
$$Y_j=e^{x_j},j=1,...,n/2. \\ Y'_j=e^{1-x_j},j=1,...,n/2. \\ \hatθ_1=\frac{1}{n}\sum_{j=1}^{n/2}(Y_j+Y'_{j})=\frac{1}{n/2}\sum_{j=1}^{n/2}\frac{Y_j+Y'_{j}}{2}$$
theoretical value:$$Var(e^x+e^{1-x})$$


```{r }
n<-1e5
x<-runif(n)
theta.hat<-mean(exp(x))
theovar.theta<-exp(1)-exp(0)
var1<-var(exp(x))/n
x1<-x[1:n/2]
y<-exp(x1)
y1<-exp(1-x1)
theta1.hat<-mean((y+y1)/2)
print(c(theta.hat,theta1.hat)) 
var2<-var((y+y1)/2)/(n/2)
theovar.theta1<-10*exp(1)-3*exp(2)-5
print((var1-var2)/var1)
print(c(theovar.theta,theovar.theta1,(theovar.theta-theovar.theta1)/theovar.theta))
```



# Exercises 5.11:
If $\hatθ_1$ and $\hatθ_2$ are unbiased estimators of $θ$, and $\hatθ_1$ and $\hatθ_2$ are antithetic, we derived that $c^* = \frac{1}{2}$ is the optimal constant that minimizes the variance of $\hatθ_c = c\hatθ_1 + (1 − c)\hatθ_2$. Derive $c^*$ for the general case. That is, if $\hatθ_1$ and $\hatθ_2$ are any two unbiased estimators of $θ$, find the value $c^∗$ that minimizes the variance of the estimator $\hatθ_c = c\hatθ_1 + (1 − c)\hatθ_2$ in equation (5.11). ($c^∗$ will be a function of the variances and the covariance of the estimators.)

Because,$$\hatθ_c = c\hatθ_1 + (1 − c)\hatθ_2$$
$$
\begin{aligned}
Var(\hatθ_c)&=c^2Var(\hatθ_1)+(1-c)^2Var(\hatθ_2)+2c(1-c)Cov(\hatθ_1,\hatθ_2)\\
&=c^2Var(\hatθ_1)+c^2Var(\hatθ_2)-2c^2Cov(\hatθ_1,\hatθ_2)+2cCov(\hatθ_1,\hatθ_2)-2cVar(\hatθ_2)+Var(\hatθ_2)\\
&=c^2Var(\hatθ_1-\hatθ_2)+2cCov(\hatθ_2,\hatθ_1-\hatθ_2)+Var(\hatθ_2)
\end{aligned}
$$
Hence,when $$c^*=\frac{Cov(\hatθ_2,\hatθ_2-\hatθ_1)}{Var(\hatθ_1-\hatθ_2)}$$

Variance$\hatθ_c$ is minimum.

## HW3

# Exercises 5.13
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,∞)$ and
are ‘close’ to$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{{-x^2}/2},x>1.$$
Which of your two importance functions should produce the smaller variance
in estimating $$\int_1^∞\frac{x^2}{\sqrt{2\pi}}e^{{-x^2}/2}dx$$ by importance sampling? Explain.

choices of importance functions $f_1$ and $f_2$:
$$f_1(x)=e^{-(x-1)},x>1$$
$$f_2(x)=\frac{2}{\sqrt{2\pi}}e^{{-(x-1)^2}/2},x>1$$
The graph with $g(x)$ in (a) and the ratios $g(x)/f_i(x)$ in (b).
```{r}
x=seq(1,5,0.01)
g <- (x^2)* exp(-(x^2)/2)/sqrt(2* pi)
f1<-exp(1-x)
f2<-(2/pi)^(1/2)*exp(-(1/2)*(x-1)^2)

#figure (a)
plot(x, g, type = "l", main = "",ylim = c(0,1.2), lwd = 2,xlab="(a)")
lines(x, f1, lty = 3, lwd = 2)
lines(x, f2, lty = 4, lwd = 2)
legend("topright", legend = c("g","f1","f2"),lty = 1:6, lwd = 2, inset = 0.02)

#figure (b)
plot(x, g, type = "l", main = "",ylim=c(0,0.8),lwd=2,lty=2,xlab="(b)")
lines(x, g/f1, lty = 3, lwd = 2)
lines(x, g/f2, lty = 4, lwd = 2)
legend("topright", legend = c("g","g/f1","g/f2"),lty = 2:6, lwd = 2, inset = 0.02)
```


The estimates and variances of $f_1$ and $f_2$ are as follows
```{r}
m <- 10000
theta.hat <- se <- numeric(2)
g <- function(x) {
(x^2)* exp(-(x^2)/2)/sqrt(2* pi) * (x > 1)
}

u<-runif(m)
x<-1-log(1-u)
fg<-g(x)/exp(1-x)
theta.hat[1]<-mean(fg)
se[1]<-sd(fg)


u<-rnorm(m,1,1)
for (i in 1:m) {
  if(u[i]<=1) {x[i]<-2-u[i]}
  else {x[i]<-u[i]} 
}
fg<-g(x)/(2*dnorm(x,1,1))
theta.hat[2]<-mean(fg)
se[2]<-sd(fg)

rbind(theta.hat,se)
```
Hence, the two importance functions are similar, and the estimated variance of $f_2$is smaller.


# Exercises 5.15
Obtain the stratified importance sampling estimate in Example 5.13 and compare
it with the result of Example 5.10.
$$\int_0^1g(x)dx=\sum_{j=1}^{5}\int_{\frac{j-1}{5}}^{\frac{j}{5}}\frac{g(x)}{f_j(x)}f_j(x)dx$$
$$f_j(x)=\frac{e^{-x}}{e^{-\frac{j-1}{5}}-e^{-\frac{j}{5}}},e^{-\frac{j-1}{5}}<x<e^{-\frac{j}{5}}$$
$$\frac{g(x)}{f_j(x)}=\frac{e^{-\frac{j-1}{5}}-e^{-\frac{j}{5}}}{1+x^2}$$
$$F_j(x)=\frac{e^{-\frac{j-1}{5}}-e^{-x}}{e^{-\frac{j-1}{5}}-e^{-\frac{j}{5}}}\ ,\  e^{-\frac{j-1}{5}}<x<e^{-\frac{j}{5}}$$

```{r}
M <- 10000 
k <- 5 
r <- M / k 
N <- 50 
T0 <- numeric(k)
estimates <- matrix(0, N, 2)
g <- function(x) {
exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}

for (i in 1:N) {
  u <- runif(M) 
  x <- - log(1 - u * (1 - exp(-1)))
  fg <- g(x) / (exp(-x) / (1 - exp(-1)))
  estimates[i, 1] <- mean(fg)

for (j in 1:k){
  u<-runif(M/k)
  x<- -log(exp(-(j-1)/5)-u*(exp(-(j-1)/5)-exp(-(j)/5)))
  fg1<-(exp(-(j-1)/5)-exp(-(j)/5))/(1+x^2)
  T0[j] <- mean(fg1)
  }
estimates[i, 2] <- mean(T0)*5
}
apply(estimates, 2, mean)
apply(estimates, 2, var)
```



# Exercises 6.4
Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95%$ confidence interval for the parameter $μ$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

here is the deduction:
$$X\sim LN(\mu,\sigma^2)$$
$$Y=LnX\sim N(\mu,\sigma^2)$$
$$\frac{\sqrt{n}(\overset{-}{Y}-\mu)}{s}\sim t(n-1)$$
the 95% confidence interval of $\mu$ is 
$$[\overset{-}{Y}-t_{0.975}(n-1)s/\sqrt{n},\overset{-}{Y}+t_{0.975}(n-1)s/\sqrt{n}]$$
The sample proportion of intervals that contain $σ^2 = 4$ is a Monte Carlo estimate of the true confidence level.
```{r}
n <- 20
UCL1 <- replicate(1000, expr = {
x <- rlnorm(n, mean = 0, sd = 2)
y<-log(x)
mean(y)-(sd(y)*qt(0.975,n-1))/sqrt(n)
} )
UCL2 <- replicate(1000, expr = {
x <- rlnorm(n, mean = 0, sd = 2)
y<-log(x)
mean(y)+(sd(y)*qt(0.975,n-1))/sqrt(n)
} )
sum(UCL1 < 0&UCL2 > 0)
mean(UCL1 < 0&UCL2 > 0)
```
Hence,the result is close to 0.95.


# Exercises 6.5
Suppose a $95%$ symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$χ^2(2)$ data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

$$X\sim χ^2(2)$$
$$\frac{\sqrt{n}(\overset{-}{X}-\mu)}{s}\sim t(n-1)$$
the 95% confidence interval of $\mu$ is 
$$[\overset{-}{X}-t_{0.975}(n-1)s/\sqrt{n},\overset{-}{X}+t_{0.975}(n-1)s/\sqrt{n}]$$

```{r}
n <- 20
UCL1 <- replicate(1000, expr = {
x <- rchisq(n,2)
mean(x)-(sd(x)*qt(0.975,n-1))/sqrt(n)
} )
UCL2 <- replicate(1000, expr = {
x <- rchisq(n,2)
mean(x)+(sd(x)*qt(0.975,n-1))/sqrt(n)
} )
c1<-mean(UCL1 < 2&UCL2 > 2)
#example 6.4
UCL <- replicate(1000, expr = {
x <- rnorm(n, mean = 0, sd = 2)
(n-1) * var(x) / qchisq(.05, df = n-1)
} )
c2<-mean(UCL > 4)
print(c(c1,c2))
```

## HW4

# Exercises 6.7
Estimate the power of the skewness test of normality against symmetric Beta(α, α) distributions and comment on the results. Are the results different
for heavy-tailed symmetric alternatives such as t(ν)?

Because the skewness sample is approximately equal to 0 and the variance is equal to $\frac{6(n-2)}{(n+1)(n+3)}$,we compare the beta distribution with the heavy-tailed -T distribution,Parameters are selected from 1 to 100 at intervals of 1.
```{r}
n<-30
m<-1e3
mu<-c(seq(1,100,1))
M<-length(mu)
cv<-qnorm(0.975,0,sqrt(6*(n-2) / ((n+1)*(n+3))))
set.seed(1234)
sk <- function(x) { 
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2) 
  return( m3 / m2^1.5 )
}
pvalues1<-numeric(M)
pvalues2<-numeric(M)
for(i in 1:M){
  skt1<-numeric(m)
  skt2<-numeric(m)
  for(j in 1:m){
  x1<-rbeta(n,mu[i],mu[i])
  skt1[j]<-as.integer(abs(sk(x1)>=cv))
  }
  pvalues1[i]<-mean(skt1)
  for(j in 1:m){
    x2<-rt(n,mu[i])
    skt2[j]<-as.integer(abs(sk(x2)>=cv))
  }
  pvalues2[i]<-mean(skt2) 
}
opar<-par(no.readonly = TRUE)
par(mfrow=c(1,2))
plot(mu,pvalues1,type = "b",pch=18,xlab = "mu(beta)",ylim=c(0,0.05))
abline(h=.05,lty=6,col="red")
plot(mu,pvalues2,type = "b",pch=18,xlab = "mu(t)",ylim=c(0,0.05))
abline(h=.05,lty=6,col="red")
par(opar)


```
Hence, the power of the skewness test of normality against symmetric Beta distributions is always below 0.05,but the estimated value of efficacy becomes larger with the increase of parameters, while the estimated value using T distribution decreases with the increase of parameters.


# Exercises 6.8
Refer to Example 6.16. Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{α}\overset{·}=0.055$. Compare the power of the Count Five test and $F$ test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.)

using 10,50,100 samples for 1000 simulations,and set the significance level as 0.055 to conduct the F test,the results are as follows:
```{r,eval=FALSE}
sigma1 <- 1 
sigma2 <- 1.5
m<-1e3
n<-c(10,50,100)

count5test <- function(x, y) {
  X <- x - mean(x) 
  Y <- y - mean(y) 
  outx <- sum(X > max(Y)) + sum(X < min(Y)) 
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(outx, outy)) > 5))
}
Ftest<-function(x,y){
  P<-var.test(x,y,ratio = 1,alternative = c("two.sided","less","greater"),
         conf.level = 0.945)
  return(P$p.value)
}
set.seed(1234)
power<-matrix(0,3,2,dimnames = list(c("10","50","100"),c("count5test","Ftest"))) 
for(i in 1:length(n)){
  for(j in 1:m){
  count5test_p<-mean(replicate(1e3,expr={
    x<-rnorm(n[i],0,sigma1)
    y<-rnorm(n[i],0,sigma2)
    count5test(x,y)
  }))
  x<-rnorm(n[i],0,sigma1)
  y<-rnorm(n[i],0,sigma2)
  Ftest_p<-Ftest(x,y)
  }
  power[i,1]<-count5test_p
  power[i,2]<-Ftest_p
}
power

```



# Exercises 6.c

```{r,eval=FALSE}
n<-c(10,20,30,50,100)
cv<-qchisq(0.975,4)
cvb<-6*cv/n
m<-100
reject_p<-numeric(length(n))
sk<-function(n){
  x<-rnorm(n)
  y<-rnorm(n)
  d=matrix(0,n,2)
  z<-numeric(n)
  for(i in 1:n){
  d[i,1]=x[i]-mean(x)
  d[i,2]=y[i]-mean(y)
  }
cov<-cov(d)
for(i in 1:n){
  for(j in 1:n){
  b<-(matrix(d[i,],1,2)%*%solve(cov)%*%t(matrix(d[j,],1,2)))^3
  }
  z[i]<-b
}
return(mean(z))
}
for(k in 1:length(n)){
  skt<-numeric(m)
  for(h in 1:m){
    skt[h]<-as.integer(abs(sk(n[k]))>=cvb[k])
  }
 reject_p[k]<-mean(skt) 
}
reject_p
```


```{r,eval=FALSE}
n<- 30 
m <-100
epsilon <- c(seq(0, 0.15, 0.01), seq(0.15, 1, 0.05))
l<-length(epsilon)
cv<-qchisq(0.975,4)
power<-numeric(l) 
sk<-function(epsilon){
  sigma<-sample(c(1, 10), replace = TRUE, size = n, prob = c(1-epsilon, epsilon))
  x<-rnorm(n,0,sigma)
  y<- rnorm(n,0,sigma)
  d=matrix(0,n,2)
  z<-numeric(n)
  for(i in 1:n){
    d[i,1]=x[i]-mean(x)
    d[i,2]=y[i]-mean(y)
  }
  cov<-cov(d)
  for(i in 1:n){
    for(j in 1:n){
      b<-(matrix(d[i,],1,2)%*%solve(cov)%*%t(matrix(d[j,],1,2)))^3
    }
    z[i]<-b
  }
  return(mean(z))
}
for (i in 1:l) {
  skt<-numeric(m)
  for(j in 1:m){
    skt[j]<-as.integer(abs(sk(epsilon[i]))>=cv) 
    }
  power[i]<-mean(skt) 
}
power
```


# Discussion
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?
1.What is the corresponding hypothesis test problem?
2.What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?
3.What information is needed to test your hypothesis?


## solve
we can say the powers are different or not after the hypothesis test.
1.Test hypothesis $$H_0 : powers1-powers2=0  ,   H_1 :powers1-powers2\neq 0$$
2.Because we obtain the powers for two methods under a particular simulation setting with 10,000 experiments,which means the samples are paired,so we shoule use paired-t test.
3.We need to know the estimates of variance are equal or not,which depends the test statistics

## HW5

# Exercises 7.1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

```{r,eval=FALSE}
library(bootstrap) #for the law data
n <- nrow(law)
y <- law$LSAT
z <- law$GPA
theta.hat <- cor(y, z)
#compute the jackknife replicates, leave-one-out estimates
theta.jack <- numeric(n)
for (i in 1:n)
   theta.jack[i] <- cor(y[-i], z[-i])
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
#jackknife estimate of bias
se <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2)) 
#Jackknife estimate of standard error
round(c(original=theta.hat,bias.jack=bias,se.jack=se),7)

```



# Exercises 7.5
Refer to Exercise 7.4.Compute 95% bootstrap confidence intervals for the mean time between failures $1/λ$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

```{r,eval=FALSE}
library(boot)
boot.obj<-boot(data=aircondit,statistic=function(x, i){mean(x[i,1])},R=2000)
print(boot.ci(boot.obj,type = c("basic", "norm", "perc","bca")))


```
The confidence intervals of each estimation method are as follows
```{r,eval=FALSE}
#calculations for bootstrap confidence intervals
alpha <- c(.025, .975)
#normal
print(boot.obj$t0 + qnorm(alpha) * sd(boot.obj$t))
#basic
print(2*boot.obj$t0 - quantile(boot.obj$t, rev(alpha), type=1))
#percentile
print(quantile(boot.obj$t, alpha, type=6))

```


# Exercises 7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{θ}$.

```{r,eval=FALSE}
library(bootstrap)
n<-nrow(scor)
lambda_hat<-eigen(cov(scor))$values
theta_hat<-lambda_hat[1]/sum(lambda_hat)
theta.jack<-numeric(n)
for (i in 1:n){
x<-scor[-i,]
lambda<-eigen(cov(x))$values
theta.jack[i] <- lambda[1] / sum(lambda)
}
bias <- (n - 1) * (mean(theta.jack) - theta_hat)
se <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2)) 
round(c(original=theta.hat,bias.jack=bias,se.jack=se),7)

```




# Exercises 7.11
In Example 7.18,leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.


```{r,eval=FALSE}
library(DAAG)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
matrix<-combn(n,2)
m<-ncol(matrix)
for (k in 1:m) {
    y <- magnetic[-matrix[,k]]
    x <- chemical[-matrix[,k]]
    
    J1 <- lm(y ~ x)
    yhat11 <- J1$coef[1] + J1$coef[2] * chemical[matrix[1,k]]
    yhat12 <- J1$coef[1] + J1$coef[2] * chemical[matrix[2,k]]
    e1[k] <- ((magnetic[matrix[1,k]] - yhat11)^2+(magnetic[matrix[2,k]] - yhat12)^2)/2
    
    J2 <- lm(y ~ x + I(x^2))
    yhat21 <- J2$coef[1] + J2$coef[2] * chemical[matrix[1,k]] +J2$coef[3] * chemical[matrix[1,k]]^2
    yhat22 <- J2$coef[1] + J2$coef[2] * chemical[matrix[2,k]] +J2$coef[3] * chemical[matrix[2,k]]^2
    e2[k] <- ((magnetic[matrix[1,k]] - yhat21)^2+(magnetic[matrix[2,k]] - yhat22)^2)/2
    
    J3 <- lm(log(y) ~ x)
    logyhat31 <- J3$coef[1] + J3$coef[2] * chemical[matrix[1,k]]
    logyhat32 <- J3$coef[1] + J3$coef[2] * chemical[matrix[2,k]]
    yhat31 <- exp(logyhat31)
    yhat32 <- exp(logyhat32)
    e3[k] <- ((magnetic[matrix[1,k]] - yhat31)^2+(magnetic[matrix[2,k]] - yhat32)^2)/2
    
    J4 <- lm(log(y) ~ log(x))
    logyhat41 <- J4$coef[1] + J4$coef[2] * log(chemical[matrix[1,k]])
    logyhat42 <- J4$coef[1] + J4$coef[2] * log(chemical[matrix[2,k]])
    yhat41 <- exp(logyhat41)
    yhat42 <- exp(logyhat42)
    e4[k] <- ((magnetic[matrix[1,k]] - yhat41)^2+(magnetic[matrix[2,k]] - yhat42)^2)/2
}

c(mean(e1), mean(e2), mean(e3), mean(e4))

L2<-lm(magnetic ~ chemical + I(chemical^2))
L2
```


#The fitted regression equation for Model 2 is
$$\hat{Y} = 24.49262− 1.39334X + 0.05452X^2$$
```{r,eval=FALSE}

par(mfrow = c(2, 2)) #layout for graphs
plot(L2$fit, L2$res) #residuals vs fitted values
abline(0, 0) #reference line
qqnorm(L2$res) #normal probability plot
qqline(L2$res) #reference line
par(mfrow = c(1, 1)) #restore display
```    

## HW6

# Exercises 8.3
```{r}
countmaxtest <- function(x, y) {
    X <- x - mean(x)
    Y <- y - mean(y)
    outx <- sum(X > max(Y)) + sum(X < min(Y))
    outy <- sum(Y > max(X)) + sum(Y < min(X))
    return(max(c(outx, outy)))
}


R <- 999 
set.seed(3)
x<-rnorm(20,0,1)
y<-rnorm(30,0,1)
z <- c(x, y) #pooled sample
K <- 1:36
D <- numeric(R) #storage for replicates
D0 <- countmaxtest(x, y)
for (i in 1:R) {    
     k <- sample(K, size = 20, replace = FALSE)
     x1 <- z[k]
     y1 <- z[-k] 
     D[i] <- countmaxtest(x1, y1)
}

p <- mean(c(D0, D) >= D0)
print(p)
hist(D, main = "", freq = FALSE, xlab = "D (p = 0.542)",breaks = "scott")
points(D0, 0, cex = 1, pch = 16)
```


# Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations.
```{r,eval=FALSE}
library(RANN)
library(boot)
library(energy)
library(Ball)
m <- 100
k<-3
p<-2
mu <- 0.3
set.seed(12345)
n1 <- n2 <- 50
R<-999
n <- n1+n2
N = c(n1,n2)
alpha<-0.1
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]
n2 <- sizes[2]
n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) 
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}

eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)

```


## 1.Unequal variances and equal expectations
```{r,eval=FALSE}
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p)
y <- cbind(rnorm(n2),rnorm(n2,mean=mu))
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}
pow1 <- colMeans(p.values<alpha)

```
## 2.Unequal variances and unequal expectations
```{r,eval=FALSE}
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p)
y <- cbind(rnorm(n2),rnorm(n2,sd=1))
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}
pow2 <- colMeans(p.values<alpha)

```

## 3.Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)
```{r,eval=FALSE}
for(i in 1:m){
x <- matrix(rt(n1*p,1),ncol=p)
y <- cbind(rt(n2,1),rt(n2,2))
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}
pow3 <- colMeans(p.values<alpha)

for(i in 1:m){
  bimodel1<-c(rnorm(n1/2,mean = mu),rnorm(n1/2,mean = -mu))
  bimodel2<-c(rnorm(n2/2,mean = mu),rnorm(n2/2,mean = -mu))
  bimodel3<-c(rnorm(n1/2,mean = mu+1),rnorm(n1/2,mean = -mu-1))
  bimodel4<-c(rnorm(n2/2,mean = mu+1),rnorm(n2/2,mean = -mu-1))
  x<-cbind(bimodel1,bimodel2)
  y<-cbind(bimodel3,bimodel4)
  z <- rbind(x,y) 
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value 
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
pow4 <- colMeans(p.values<alpha)

```
## 4.Unbalanced samples (say, 1 case versus 10 controls)
```{r,eval=FALSE}
p1<-10
for(i in 1:m){ 
  x <- matrix(rnorm(n1*p1),ncol = p1)
  y <- cbind(matrix(rnorm(n2*(p1-1)),ncol=p1-1),rnorm(n2,mean=mu))
  z <- rbind(x,y) 
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value 
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
pow5<- colMeans(p.values<alpha)

```
##5.Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).
```{r,eval=FALSE}
rnames=c("unequal variences","unequal variences and  expectations","T-distribution","bimodel distribution","Unbalanced samples")
cnames<-c("NN","energy","ball")
matrix(c(pow1,pow2,pow3,pow4,pow5),5,3,dimnames = list(rnames,cnames),byrow = T)



```

## HW7

# Exercies 9.4
choose the sigma of 0.05,0.05,1,16 to simulate
```{r,eval=FALSE}
rw.Metropolis <- function(n, sigma, x0, N) {
   x <- numeric(N)
   x[1] <- x0
   u <- runif(N)
   k <- 0
   for (i in 2:N) {
       y <- rnorm(1, x[i-1], sigma)
       if (u[i] <= (exp(-abs(y))/exp(-abs(x[i-1]))))
       x[i] <- y else {
           x[i] <- x[i-1]
           k <- k + 1
       }
   }
return(list(x=x, k=k))
}

n <- 4 
N <- 2500
sigma <- c(.05, .5, 1, 16)
x0 <- 25
rw1 <- rw.Metropolis(n, sigma[1], x0, N)
rw2 <- rw.Metropolis(n, sigma[2], x0, N)
rw3 <- rw.Metropolis(n, sigma[3], x0, N)
rw4 <- rw.Metropolis(n, sigma[4], x0, N)

print(c(1-rw1$k/N, 1-rw2$k/N, 1-rw3$k/N, 1-rw4$k/N))

 par(mfrow=c(2,2))  
    rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
    for (j in 1:4) {
        plot(rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
    }
    par(mfrow=c(1,1))
```

# Exercise 9.4, use the Gelman-Rubin method
```{r}
Gelman.Rubin <- function(psi) {
   psi <- as.matrix(psi)
   n <- ncol(psi)
   k <- nrow(psi)
   psi.means <- rowMeans(psi) #row means
   B <- n * var(psi.means) #between variance est.
   psi.w <- apply(psi, 1, "var") #within variances
   W <- mean(psi.w) #within est.
   v.hat <- W*(n-1)/n + (B/n) #upper variance est.
   r.hat <- v.hat / W #G-R statistic
   return(r.hat)
}

rw.Metropolis <- function( sigma, x0, N) {
   x <- numeric(N)
   x[1] <- x0
   u <- runif(N)
   k <- 0
   for (i in 2:N) {
       y <- rnorm(1, x[i-1], sigma)
       if (u[i] <= (exp(-abs(y))/exp(-abs(x[i-1]))))
       x[i] <- y else {
           x[i] <- x[i-1]
           k <- k + 1
       }
   }
return(x)
}

```

### sigma=0.05
```{r,eval=FALSE}
sigma <- .05
k <- 4 
N <- 15000 
b <- 1000 

x0 <- c(-10, -5, 5, 10)
X <- matrix(0, nrow=k, ncol=N)
for (i in 1:k)
   X[i, ] <- rw.Metropolis(sigma, x0[i], N)
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
   psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi[i, (b+1):N], type="l",xlab=i, ylab=bquote(psi))

par(mfrow=c(1,1))
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="sigma=0.05", ylab="R")
abline(h=1.1, lty=2)

```

### sigma=0.5
```{r,eval=FALSE}
sigma <- .5

x0 <- c(-10, -5, 5, 10)
X <- matrix(0, nrow=k, ncol=N)
for (i in 1:k)
   X[i, ] <- rw.Metropolis(sigma, x0[i], N)
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
   psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi[i, (b+1):N], type="l",xlab=i, ylab=bquote(psi))

par(mfrow=c(1,1))
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="sigma=0.5", ylab="R")
abline(h=1.1, lty=2)

```

### sigma=1
```{r,eval=FALSE}
sigma <- 1

x0 <- c(-10, -5, 5, 10)
X <- matrix(0, nrow=k, ncol=N)
for (i in 1:k)
   X[i, ] <- rw.Metropolis(sigma, x0[i], N)
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
   psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi[i, (b+1):N], type="l",xlab=i, ylab=bquote(psi))

par(mfrow=c(1,1))
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="sigma=1", ylab="R")
abline(h=1.1, lty=2)

```

### sigma=16
```{r,eval=FALSE}
sigma <-16

x0 <- c(-10, -5, 5, 10)
X <- matrix(0, nrow=k, ncol=N)
for (i in 1:k)
   X[i, ] <- rw.Metropolis(sigma, x0[i], N)
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
   psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

par(mfrow=c(2,2))
for (i in 1:k)
  plot(psi[i, (b+1):N], type="l",xlab=i, ylab=bquote(psi))

par(mfrow=c(1,1))
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="sigma=16", ylab="R")
abline(h=1.1, lty=2)

```


# Exercises 11.4
```{r}
y<-function(k){
  s<-function(a){
    s1<-1-pt((a^2*(k-1)/(k-a^2))^0.5,k-1)
    s2<-1-pt((a^2*k/(k+1-a^2))^0.5,k)
    s1-s2
  }
  return(uniroot(s, interval = c(0.01, sqrt(k)-0.01))$root) 
}
res = sapply(c(4:25, 100, 500, 1000),y)
print(res)


```


## HW8

# A-B-O blood type problem



* Let the three alleles be A, B, and O.

| Genotype  | AA     | BB     | OO     | AO     | BO     | AB     | Sum |
|-----------|--------|--------|--------|--------|--------|--------|-----|
| Frequency | $p^2$    | $q^2$    | $r^2$    | 2pr    | 2qr    | 2pq    | 1   |
| Count     | $n_{AA}$ | $n_{BB}$ | $n_{OO}$ | $n_{AO}$ | $n_{BO}$ | $n_{AB}$ | n   |

* Observed data: $n_{A\cdot}=n_{AA}+n_{AO}=444$(A-type), $n_{B\cdot}=n_{BB}+n_{BO}=132$(B-type), $n_{OO}=361$(O-type), $n_{AB}=63$(AB-type)

* Use EM algorithm to solve MLE of p and q (consider missing data $n_{AA}$ and $n_{BB}$).

* Record the values of p and q that maximize the conditional likelihood in each EM steps, calculate the corresponding log-maximum likelihood values (for observed data), are they increasing?

### Answer
Let$\theta=(p_{AA},p_{AO},p_{BB},p_{BO},p_{OO},p_{AB})$, where$(p_{AA},p_{AO},p_{BB},p_{BO},p_{OO},p_{AB})=(p^2,2pr,q^2,2qr,r^2,2pq)$.
then the likelihood function is： 
\begin{equation*}
\begin{split}
L(\theta|n_{AA},n_{AO},n_{BB},n_{BO},n_{OO},n_{AB},\theta) 
& = {(p^2)}^{n_{AA}}{(2pr)}^{n_{AO}}{(q^2)}^{n_{BB}}{(2qr)}^{n_{BO}}{(r^2)}^{n_{OO}}{(2pq)}^{n_{AB}}\frac{n!}{n_{AA}!n_{AO}!n_{BB}!n_{BO}!n_{OO}!n_{AB}!}.
\end{split}
\end{equation*}
Because$n_{A\cdot},n_{B\cdot},n_{OO},n_{AB}$ are known,but$n_{AA},n_{AO},n_{BB},n_{BO}$is unknown,thus, in step t of EM algorithm:
\[n_{AA}^{(t)},n_{AO}^{(t)}|n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)}\sim MN(n_{A\cdot},\frac{{(p^{(t)})}^2}{{(p^{(t)})}^2+2p^{(t)}r^{(t)}},\frac{2p^{(t)}r^{(t)}}{{(p^{(t)})}^2+2p^{(t)}r^{(t)}})\]
\[n_{BB}^{(t)},n_{BO}^{(t)}|n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)}\sim MN(n_{B\cdot},\frac{{(q^{(t)})}^2}{{(q^{(t)})}^2+2q^{(t)}r^{(t)}},\frac{2q^{(t)}r^{(t)}}{{(q^{(t)})}^2+2q^{(t)}r^{(t)}})\]
In step t of EM algorithm,the logarithm of the complete likelihood is obtained by:
\begin{equation*}
\begin{split}
Q(\theta|\theta^{(t)}) 
 & = N_{AA}^{(t)}ln(p^2)+N_{AO}^{(t)}ln(2pr)+N_{BB}^{(t)}ln(q^2)+N_{BO}^{(t)}ln(2qr)+N_{OO}^{(t)}ln(r^2)+N_{AB}^{(t)}ln(2pq)+k(n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)})
\end{split}
\end{equation*}
where\[N_{AA}^{(t)}=E(n_{AA}^{(t)}|n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)})=n_{A\cdot}\cdot\frac{{(p^{(t)})}^2}{{(p^{(t)})}^2+2p^{(t)}r^{(t)}}\]
\[N_{AO}^{(t)}=E(n_{AO}^{(t)}|n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)})=n_{A\cdot}\cdot\frac{2p^{(t)}r^{(t)}}{{(p^{(t)})}^2+2p^{(t)}r^{(t)}}\]
\[N_{BB}^{(t)}=E(n_{BB}^{(t)}|n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)})=n_{B\cdot}\cdot\frac{{(q^{(t)})}^2}{{(q^{(t)})}^2+2q^{(t)}r^{(t)}}\]
\[N_{BO}^{(t)}=E(n_{BO}^{(t)}|n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)})=n_{B\cdot}\cdot\frac{2q^{(t)}r^{(t)}}{{(q^{(t)})}^2+2q^{(t)}r^{(t)}}\]
\[N_{OO}^{(t)}=n_{OO},\ N_{AB}^{(t)}=n_{AB} \]
\[k(n_{A\cdot},n_{B\cdot},n_{OO},n_{AB},\theta^{(t)})=\frac{n!}{n_{AA}!n_{AO}!n_{BB}!n_{BO}!n_{OO}!n_{AB}!} \]
In this paper, we maximize the value of $Q(\theta|\theta^{(t)})$,because$p+q+r=1$,the derivation of$p,q$ is:
\[\frac{\partial Q(\theta|\theta^{(t)})}{\partial p}=\frac{2N_{AA}^{(t)}+N_{AO}^{(t)}+N_{AB}^{(t)}}{p}-\frac{N_{AO}^{(t)}+N_{BO}^{(t)}+2N_{OO}^{(t)}}{1-p-q} \]
\[\frac{\partial Q(\theta|\theta^{(t)})}{\partial q}=\frac{2N_{BB}^{(t)}+N_{BO}^{(t)}+N_{AB}^{(t)}}{q}-\frac{N_{AO}^{(t)}+N_{BO}^{(t)}+2N_{OO}^{(t)}}{1-p-q} \]
Let the derivative be 0, then:
\[p^{(t+1)}= \frac{2N_{AA}^{(t)}+N_{AO}^{(t)}+N_{AB}^{(t)}}{2n}\]
\[q^{(t+1)}= \frac{2N_{BB}^{(t)}+N_{BO}^{(t)}+N_{AB}^{(t)}}{2n}\]
\[r^{(t+1)}=\frac{2N_{OO}^{(t)}+N_{AO}^{(t)}+N_{BO}^{(t)}}{2n} \]
Then EM algorithm can be used to solve the MLE of $p,q$:
```{r}
ABO.em<-function(p.ini,n.obs){
  M=1e4 #maximum ierations
  tol=.Machine$double.eps #when to converge

  n=sum(n.obs)
  nA.=n.obs[1]
  nB.=n.obs[2]
  nOO=n.obs[3]
  nAB=n.obs[4]
  
  p=q=r=numeric(0)
  loglikelihood=numeric(0)
  p[1]=p.ini[1]
  q[1]=p.ini[2]
  r[1]=1-p[1]-q[1]
  loglikelihood[1]=0
  iter=1
  
  for(i in 2:M){
    p.old=p[i-1]
    q.old=q[i-1]
    r.old=r[i-1]
    
    nAA.t=nA.*p.old^2/(p.old^2+2*p.old*r.old)
    nAO.t=nA.*2*p.old*r.old/(p.old^2+2*p.old*r.old)
    nBB.t=nB.*q.old^2/(q.old^2+2*q.old*r.old)
    nBO.t=nB.*2*q.old*r.old/(q.old^2+2*q.old*r.old)
    nOO.t=nOO
    nAB.t=nAB
    
    p[i]=(2*nAA.t+nAO.t+nAB.t)/2/n
    q[i]=(2*nBB.t+nBO.t+nAB.t)/2/n
    r[i]=(2*nOO.t+nAO.t+nBO.t)/2/n
    iter=iter+1
    
    loglikelihood[i]=nAA.t*2*log(p[i])+nAO.t*log(2*p[i]*r[i])+nBB.t*2*log(q[i])+nBO.t*log(q[i]*r[i])+nOO.t*2*log(r[i])+nAB.t*log(2*p[i]*q[i])
    
    U=abs((p[i]-p.old)/p.old)<=tol
    V=abs((q[i]-q.old)/q.old)<=tol
    W=abs((r[i]-r.old)/r.old)<=tol
    if(U&&V&&W)
      break
  }
  list(p.mle.em=p[iter],q.mle.em=q[iter],r.mle.em=r[iter],iter=iter,p.mle.all=p,q.mle.all=q,loglikelihoods=loglikelihood)
}
nObs=c(444,132,361,63)
pInitial=c(1/3,1/3) #initial p,q value
em.result<-ABO.em(p.ini=pInitial,n.obs=nObs)

print(c(em.result[1],em.result[2],em.result[3]))

par(mfrow=c(1,2))
plot(em.result$p.mle.all,xlab = "iter",ylab = "p.mle",type="b",ylim = c(0,0.4))

plot(em.result$q.mle.all,xlab = "iter",ylab = "q.mle",type="b",ylim=c(0,0.4))

plot(em.result$loglikelihoods[-1],xlab = "iter",ylab = "loglikehood",type="b")
```

Record the MLE values of $p,q$ after each iteration, and the corresponding log likelihood function values (ignoring the constant term independent of $\theta$ in $logL(\theta |n_{AA},n_{AO},n_{BB},n_{BO},n_{OO},n_{AB},\theta)$. It can be seen from the graph that the value of log likelihood (approximation) increases gradually with the increase of iteration times, and finally tends to be a constant value, which indicates that the EM algorithm is effective and converges eventually.The results of EM algorithm show that the initial values of $p,q$ are $\frac{1}{3}$. After 23 iterations, the MLE estimates of $p,q$ converge, and the corresponding estimates are 0.2976 and 0.1027, respectively.


# Exercies 3(page 204)

Use both $for\ loops$ and $lapply()$ to fit linear models to the $mtcars$ using the formulas stored in this list:
```{r}
formulas<-list(
  mpg~disp,
  mpg~I(1/disp),
  mpg~disp+wt,
  mpg~I(1/disp)+wt
)
```

```{r}
#for loop
for (i in 1:length(formulas)) {
  mod<-lm(formulas[[i]],mtcars)
  print(mod)
}

#lapply()
lapply(formulas, function(x) lm(data=mtcars,x))
```



# Exercies 3(page 213)
```{r}
trials <- replicate(
   100,
   t.test(rpois(10, 10), rpois(7, 10)),
   simplify = FALSE
)
#Use sapply()
sapply(trials,function(x) x$p.value)
#Extra challenge: get rid of the anonymous function by using [[ directly.
sapply(trials,"[[",3)
```


# Exercies 6(page 214)

Implement a combination of $Map()$ and $vapply()$ to create an $lapply()$ variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?
```{r,eval=FALSE}
myapply<-function(data,f,output.type){
  tmp<-Map(f,data)
  vapply(tmp,function(x) x ,output.type)
}

##Example
myapply(mtcars,mean,double(1))

```

## HW9

## comparison of the computation time

```{r,eval=FALSE}
    library(Rcpp)
    library(microbenchmark)
    # R
    lap_f <- function(x) exp(-abs(x))

    rw.Metropolis <- function(sigma, x0, N){
    x = numeric(N)
    x[1] <- x0
    u <- runif(N)
    k <- 0
    for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y 
    else {
    x[i] <- x[i-1]
    k <- k+1
     }
    }
     return(list(x = x, k = k))
    }

    dir_cpp = 'C:/Users/xxc/Desktop/Statistical computing/A-20030-2020-12-1/'
    sourceCpp(paste0(dir_cpp,"rwMetropolis.cpp"))
    x0 <- 25
    N <- 2500
    sigma <- 2
    (time <- microbenchmark(rwR=rw.Metropolis(sigma,x0,N),rwC=rwMetropolis(sigma,x0,N)))
```

It can be seen that the running time of CPP function is much shorter than that of R function, so the calculation efficiency can be improved by using RCP method.

## qqplot

```{r,eval=FALSE}

set.seed(3)
rwR <- rw.Metropolis(sigma,x0,N)$x[-(1:500)]
rwC <- rwMetropolis(sigma,x0,N)[-(1:500)]
qqplot(rwR,rwC)
abline(a=0,b=1,col='black')
```

Because the point of qqplot is near the diagonal, the random numbers generated by the two functions are similar.

